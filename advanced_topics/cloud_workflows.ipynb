{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafba4cd-59f6-4353-b480-e2e40235a660",
   "metadata": {},
   "source": [
    "# Cloud Workflows\n",
    "\n",
    "Cloud and High-Performance Computing (HPC) workflows can handle complex computational tasks. However, they differ significantly in their architecture, scalability, and use cases. Cloud workflows leverage cloud computing resources to execute tasks across distributed environments by utilizing virtual machines, containers, and serverless architectures. The cloud approach provides flexibility, scalability, and cost-efficiency because resources can be dynamically allocated and scaled based on demand. Cloud workflows provide easy access to shared resources and data over the internet, which makes them well-suited for collaborative tasks shared by geographically dispersed teams. Additionally, cloud platforms integrate with a wide range of services and tools, facilitating seamless workflow automation, data analysis, and machine learning.\n",
    "\n",
    "In contrast, HPC workflows are designed to maximize computational performance through tightly coupled, high-performance hardware configurations, such as supercomputers or computing clusters. HPC systems are optimized for executing large-scale, computationally intensive tasks with high efficiency and speed. This makes them ideal for scientific simulations, complex modeling, and data-intensive research. Unlike cloud workflows, which prioritize flexibility and ease of access, HPC workflows are built on raw computational power and low-latency communication between processing nodes. This requires specialized hardware and software configurations, as well as expertise in parallel programming and performance optimization. HPC workflows can deliver optimized performance for specific types of workloads, they can lack the elasticity and cost-effectiveness of cloud-based solutions. The choice between cloud and HPC workflows depends on the specific requirements of the computational tasks, including factors such as scalability, performance, cost, and ease of collaboration.\n",
    "\n",
    "Here is a comparison of cloud workflows and HPC workflows.\n",
    "\n",
    "**Cloud Computing**\n",
    "\n",
    "Cloud computing provides on-demand access to computing resources over the internet, enabling scalability, flexibility, and cost efficiency. Key features include:\n",
    "\n",
    "- Elasticity: Resources can be scaled up or down based on demand.\n",
    "- Pay-as-you-go pricing: Users pay only for the resources they consume.\n",
    "- Managed services: Cloud providers handle maintenance, security, and updates.\n",
    "- Broad accessibility: Services are available remotely via the internet.\n",
    "- Common cloud computing use cases include web hosting, SaaS applications, and - general-purpose workloads.\n",
    "\n",
    "**High Performance Computing (HPC)**\n",
    "\n",
    "HPC focuses on solving complex computational problems by leveraging powerful hardware and parallel processing. Key characteristics include:\n",
    "\n",
    "- High-speed processing: Uses specialized hardware like NVIDIA GPUs (e.g., A100, H100) for accelerated performance.\n",
    "- Low-latency networking: Optimized interconnects minimize communication delays.\n",
    "- Batch processing: Large jobs are often executed in scheduled batches.\n",
    "- Specialized workloads: Designed for simulations, AI training, and scientific research.\n",
    "\n",
    "## Data Gravity and Choosing Between Cloud and HPC\n",
    "\n",
    "Data gravity is a concept asserts that as data accumulates, it attracts additional data, services, and applications. In cloud computing, the larger and more valuable a dataset becomes, the more it attracts related data and computational processes. Data gravity spotlights the relationship between data locality and the efficient data processing. Moving large datasets across networks is time-consuming and costly. By placing computational resources close to the data, performance improves, latency is reduced, and the cost of data egress from cloud storage decreases. Large datasets influence architectural choices in cloud computing, and as a corollary, the volume of data changes scientific workflows.\n",
    "\n",
    "The default choice between cloud and HPC is run wherever you have resources But if you have choices, run where your data is. If the data is located on premise and on NFS use HPC. If youâ€™re processing data that is hosted on AWS or another cloud provider, then run on the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b129545-fedc-451a-9c12-007469425a22",
   "metadata": {},
   "source": [
    "## Methods and Tools for Cloud Workflows\n",
    "\n",
    "Parallelization, or concurrent programming, in scientific computing is the technique of dividing computational tasks into smaller subtasks that can be executed simultaneously across multiple processing units. It is an efficient way to work with data in the cloud. There are several ways to implement parallelization, ranging from pure Python solutions to [Dask](https://www.dask.org/), a distributed task scheduler. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e637bc6-c2df-4ecc-a3eb-2f190effbbbc",
   "metadata": {},
   "source": [
    "### Parallel Processing and Concurrency with Python\n",
    "\n",
    "Python implements parallel and concurrent programming through multiprocessing and multithreading modules. Each module serves a distinct purpose for a class of tasks. The multiprocessing module enables parallelism by using multiple processors or cores, which allows executing multiple processes simultaneously. This is advantageous for CPU-bound tasks, where the workload can be distributed across several processes to significantly enhance performance and reduce execution time. In contrast, the multithreading module is designed for operations that involve waiting for external events, such as data I/O or network responses. Multithreading creates multiple threads to run concurrently within the same process, making it efficient for tasks that require frequent waiting. However, due to Python's [Global Interpreter Lock (GIL)](https://wiki.python.org/moin/GlobalInterpreterLock), threads are not truly parallel but rather interleaved, which limits performance gains for CPU-bound tasks. Together, these modules optimize application performance through parallel and concurrent execution, catering to different types of computational challenges.\n",
    "\n",
    "| Feature           | Concurrency                                       | Parallelism                                       |\n",
    "|------------------|---------------------------------------------------|--------------------------------------------------|\n",
    "| **Definition**    | Managing multiple tasks at once (interleaving)    | Executing multiple tasks simultaneously          |\n",
    "| **Execution**     | Tasks appear to run at the same time              | Tasks actually run at the same time              |\n",
    "| **CPU Usage**     | Can be done on a single core (via context switching) | Requires multiple cores or processors        |\n",
    "| **Purpose**       | Maximize responsiveness, resource efficiency      | Maximize performance, throughput                 |\n",
    "| **Example**       | Switching between tasks while waiting for I/O     | Performing computations on multiple cores        |\n",
    "| **Analogy**       | One cook managing several dishes sequentially     | Multiple cooks preparing multiple dishes at once |\n",
    "| **Implementation**| Threads, coroutines, async/await                  | Multiprocessing, GPU computation, distributed systems |\n",
    "\n",
    "#### Multiprocessing\n",
    "\n",
    "Multiprocessing runs many processes across multiple CPU cores that do not share the resources among them. Each process can have many threads running in its own memory space. In Python, each process has its own instance of Python interpreter doing the job of executing the instructions. Multiprocessing can be faster for CPU-bound tasks and more robust as a crash in one process doesn't affect others. However it has higher overhead costs, slower startup time, and more complex data sharing between processes. \n",
    "\n",
    "The diagram below illustrates how multiprocessing works when performing line counts on multiple text files.\n",
    "\n",
    "```\n",
    "import multiprocessing\n",
    "\n",
    "def count_lines(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return len(file.readlines())\n",
    "\n",
    "files = [\"file1.txt\", \"file2.txt\", \"file3.txt\"]\n",
    "\n",
    "with multiprocessing.Pool() as pool:\n",
    "    line_counts = pool.map(count_lines, files)\n",
    "\n",
    "print(line_counts)\n",
    "\n",
    "\n",
    "\n",
    "                             +-------------------------+\n",
    "                             |     Main Program        |\n",
    "                             +-------------------------+\n",
    "                                        |\n",
    "                                        v\n",
    "                             +-------------------------+\n",
    "                             |  multiprocessing.Pool() |\n",
    "                             +-------------------------+\n",
    "                                        |\n",
    "              ---------------------------------------------------\n",
    "              |                         |                         |\n",
    "              v                         v                         v\n",
    "     +----------------+       +----------------+       +----------------+\n",
    "     | Worker Process |       | Worker Process |       | Worker Process |\n",
    "     |   count_lines  |       |   count_lines  |       |   count_lines  |\n",
    "     | \"file1.txt\"    |       | \"file2.txt\"    |       | \"file3.txt\"    |\n",
    "     +----------------+       +----------------+       +----------------+\n",
    "              |                         |                         |\n",
    "              v                         v                         v\n",
    "      +---------------+       +---------------+       +---------------+\n",
    "      | Line Count: X |       | Line Count: Y |       | Line Count: Z |\n",
    "      +---------------+       +---------------+       +---------------+\n",
    "              \\                         |                         /\n",
    "               \\                        |                        /\n",
    "                \\_______________________|_______________________/\n",
    "                                        v\n",
    "                          +-----------------------------+\n",
    "                          | line_counts = [X, Y, Z]     |\n",
    "                          +-----------------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09495667-ed07-4421-9e76-8a20e989151f",
   "metadata": {},
   "source": [
    "#### Multithreading \n",
    "\n",
    "A thread is the smallest unit of executionMultithreading is a technique where multiple threads are spawned by a process to do different tasks, at about the same time, just one after the other. This gives you the illusion that the threads are running in parallel, but they are actually run in a concurrent manner. In Python, the Global Interpreter Lock (GIL) prevents the threads from running simultaneously.\n",
    "\n",
    "\n",
    "good for external\n",
    "the GIL\n",
    "\n",
    "Concurrency within a single process: Threads run within a single process, sharing the same memory space.\n",
    "Global Interpreter Lock (GIL): Python's GIL restricts the execution of only one thread at a time, even on multi-core systems, thereby limiting true parallelism.\n",
    "I/O-bound tasks: Best suited for tasks involving I/O operations (like network requests, file access) where threads spend most of their time waiting for external resources, notes Built In. The GIL is released during these waiting periods, allowing other threads to run concurrently.\n",
    "Pros: Lower overhead, faster startup time, and simpler data sharing between threads.\n",
    "Cons: Limited parallelism due to GIL, can be slower for CPU-bound tasks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29094a26-c19a-4182-8a63-1ea05fb06b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import tqdm\n",
    "\n",
    "AWS_BUCKET = \"my-bucket\"\n",
    "OUTPUT_DIR = \"downloads\"\n",
    "\n",
    "def download_one_file(bucket: str, output: str, client: boto3.client, s3_file: str):\n",
    "    \"\"\"\n",
    "    Download a single file from S3\n",
    "    Args:\n",
    "        bucket (str): S3 bucket where images are hosted\n",
    "        output (str): Dir to store the images\n",
    "        client (boto3.client): S3 client\n",
    "        s3_file (str): S3 object name\n",
    "    \"\"\"\n",
    "    client.download_file(\n",
    "        Bucket=bucket, Key=s3_file, Filename=os.path.join(output, s3_file)\n",
    "    )\n",
    "\n",
    "\n",
    "files_to_download = [\"file_1\", \"file_2\", ..., \"file_n\"]\n",
    "# Creating only one session and one client\n",
    "session = boto3.Session()\n",
    "client = session.client(\"s3\")\n",
    "# The client is shared between threads\n",
    "func = partial(download_one_file, AWS_BUCKET, OUTPUT_DIR, client)\n",
    "\n",
    "# List for storing possible failed downloads to retry later\n",
    "failed_downloads = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Downloading images from S3\", total=len(files_to_download)) as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        # Using a dict for preserving the downloaded file for each future, to store it as a failure if we need that\n",
    "        futures = {\n",
    "            executor.submit(func, file_to_download): file_to_download for file_to_download in files_to_download\n",
    "        }\n",
    "        for future in as_completed(futures):\n",
    "            if future.exception():\n",
    "                failed_downloads.append(futures[future])\n",
    "            pbar.update(1)\n",
    "if len(failed_downloads) > 0:\n",
    "    print(\"Some downloads have failed. Saving ids to csv\")\n",
    "    with open(\n",
    "        os.path.join(OUTPUT_DIR, \"failed_downloads.csv\"), \"w\", newline=\"\"\n",
    "    ) as csvfile:\n",
    "        wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(failed_downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405daa5-7b95-4c80-83a5-53167f804aa9",
   "metadata": {},
   "source": [
    "# Choosing Multiprocessing vs Multithreading\n",
    "\n",
    "If your application involves a lot of I/O operations and waiting (e.g., fetching data from a web server or database), multithreading is often the better choice. If your application is CPU-intensive and performs heavy computations, multiprocessing is generally preferred to leverage multiple CPU cores. In essence: Multithreading helps hide latency by allowing concurrent execution, while multiprocessing helps speed up CPU-intensive tasks through parallel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8699bcd-3167-41c8-944a-a2bcdfbea723",
   "metadata": {},
   "source": [
    "### Workflow Management\n",
    "\n",
    "Workflow management is essential for organizing and optimizing the sequence of tasks within any process, ensuring efficiency and effectiveness in achieving desired outcomes. At its core, workflow management involves designing, executing, and monitoring a series of tasks or activities that transform inputs into outputs. A key concept in this domain is the use of directed acyclic graphs (DAGs), which provide a visual and structural framework for representing workflows. In a DAG, tasks are depicted as nodes, and the directed edges between these nodes define the dependencies and flow of tasks, ensuring that each task is executed only after its prerequisites are completed. This acyclic property prevents circular dependencies, guaranteeing that workflows progress in a logical and orderly manner. By leveraging DAGs, workflow management systems can efficiently schedule tasks, allocate resources, and handle dependencies, thereby streamlining complex processes and enhancing overall productivity. The clarity and structure provided by DAGs make them invaluable for modeling workflows in various fields, from data processing and scientific computing to business process management.\n",
    "\n",
    "#### Directed Acyclic Graphs (DAG)\n",
    "\n",
    "In this section, you're introduced to **workflow parallelization** using Directed Acyclic Graphs (DAGs), a foundational concept in scalable computing with libraries like `dask`. DAGs allow us to break down complex, multi-step workflows into discrete, interdependent tasks â€” making it easier to identify which processes can be executed in parallel rather than sequentially.\n",
    "\n",
    "You'll start by visualizing the parallelization potential in a typical waveform processing pipeline. This example mimics a simplified seismic analysis using data from multiple stations, and demonstrates how task-level parallelism is encoded using a DAG.\n",
    "\n",
    "A DAG represents tasks (nodes) and their dependencies (edges) in a one-way structure â€” no loops, no cycles. This allows us to:\n",
    "- Visually understand task order and dependencies.\n",
    "- Identify opportunities for parallel execution.\n",
    "- Optimize computation by minimizing idle time.\n",
    "\n",
    "In the context of the `trad_vs_cloud-SAGE.ipynb` notebook, this DAG represents a pipeline where:\n",
    "- Waveforms from multiple stations are processed.\n",
    "- Shared preprocessing steps (like inventory loading) feed into independent station-specific tasks.\n",
    "- Results eventually converge into common postprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b47fed6-effb-4345-9a6e-fa76df9c26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Divide  workflow into different tasks\n",
    "Task1 = 'Inventory'\n",
    "Task2_1, Task2_2, Task2_3 = 'station 1', 'station 2', 'station 3' # suppose we are accessing waveforms for three stations\n",
    "Task3 = 'Correction'\n",
    "Task4 = 'Picking'\n",
    "Task5 = 'Trimming'\n",
    "Task6 = 'Frequency'\n",
    "\n",
    "# Add nodes for tasks\n",
    "G.add_node(Task1) \n",
    "G.add_node(Task2_1), G.add_node(Task2_2), G.add_node(Task2_3)  \n",
    "G.add_node(Task3)\n",
    "G.add_node(Task4)\n",
    "G.add_node(Task5)\n",
    "G.add_node(Task6)\n",
    "# Add edges to show dependencies\n",
    "G.add_edges_from([(Task1, Task2_1), (Task1, Task2_2), (Task1, Task2_3),\n",
    "                  (Task2_1, Task3), (Task2_2, Task3), (Task2_3, Task3),\n",
    "                  (Task3, Task4), (Task4, Task5), (Task5, Task6)])\n",
    "\n",
    "# annotate each node with its stage\n",
    "layers = {\n",
    "    Task1:         0,\n",
    "    Task2_1:       1,\n",
    "    Task2_2:       1,\n",
    "    Task2_3:       1,\n",
    "    Task3:         2,\n",
    "    Task4:         3,\n",
    "    Task5:         4,\n",
    "    Task6:         5\n",
    "}\n",
    "\n",
    "for n, L in layers.items():\n",
    "    G.nodes[n]['layer'] = L\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "pos = nx.multipartite_layout(G, subset_key='layer', align='vertical')\n",
    "nx.draw(G, pos,\n",
    "        with_labels=True,\n",
    "        node_size=3000,\n",
    "        node_color=\"skyblue\",\n",
    "        font_size=10,\n",
    "        font_weight=\"bold\",\n",
    "        arrowsize=20)\n",
    "plt.title(\"Dask Task Graph (linear multipartite layout)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f165fde-1264-4b66-8bf7-8a0baa274dfd",
   "metadata": {},
   "source": [
    "This example serves as a realistic analog of seismic workflows in cloud environments, where tasks such as waveform retrieval and preprocessing for multiple stations can be executed independently â€” a classic case of what's known as an **embarrassingly parallel** problem. In such problems, individual tasks require little to no communication with one another, making them ideal candidates for concurrent execution across distributed computing resources. In the context of seismology, each station's data can be fetched, corrected, and processed in isolation before being merged downstream for higher-level analysis. By leveraging cloud-based parallelization frameworks like Dask, we can significantly reduce total computation time through intelligent task scheduling and resource allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cca861-a2ba-4c1a-812e-8a5f13af1f67",
   "metadata": {},
   "source": [
    "#### Dask Intro\n",
    "\n",
    "Dask is a Python parallel computing library designed to scale from a single laptop to a cluster of machines, Dask provides several key advantages that enhance computational efficiency and productivity:\n",
    "\n",
    "Scalability: Dask can scale your computations from a local machine to a distributed cluster with minimal code changes, allowing you to handle larger-than-memory datasets and complex computations effortlessly.\n",
    "\n",
    "Parallel Computing: By leveraging multi-core processors and distributed computing, Dask significantly speeds up data processing tasks, making it ideal for large-scale scientific computations.\n",
    "\n",
    "Integration with Existing Tools: Dask works seamlessly with popular Python libraries such as NumPy, Pandas, and Scikit-Learn. This compatibility allows users to leverage familiar APIs while benefiting from Dask's parallel computing capabilities.\n",
    "\n",
    "Lazy Evaluation: Dask uses lazy evaluation to optimize computation graphs, executing only the necessary computations and reducing redundant operations. This approach improves efficiency and performance.\n",
    "\n",
    "Flexible Data Structures: Dask provides high-level abstractions like Dask Arrays, DataFrames, and Bags that mimic NumPy arrays, Pandas DataFrames, and Python lists, respectively. These abstractions make it easier to work with large datasets in a distributed environment.\n",
    "\n",
    "Fault Tolerance: Dask's distributed scheduler includes mechanisms for fault tolerance, ensuring that computations can continue even if individual tasks fail. This robustness is crucial for long-running scientific computations.\n",
    "\n",
    "Ease of Use: With its intuitive API and comprehensive documentation, Dask is accessible to both beginners and experienced users. Its design philosophy emphasizes simplicity and ease of use, making it a practical choice for scientific computing.\n",
    "\n",
    "Community and Ecosystem: Dask benefits from a vibrant and growing community of users and developers. This active ecosystem ensures continuous improvements, extensive support, and a wealth of resources for troubleshooting and learning.\n",
    "\n",
    "In summary, Dask is a powerful and versatile tool that addresses many of the challenges associated with scientific computing. Its ability to scale, integrate with existing tools, and optimize computations makes it an essential library for researchers and data scientists working with large datasets and complex computational tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
