{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1c2659-11c1-4f59-9903-9681a4bb8434",
   "metadata": {},
   "source": [
    "# Accessing Data from EarthScope Web Services\n",
    "\n",
    "![](images/Web_Services_Data_Flow.png)\n",
    "\n",
    "\n",
    "## Getting Seismic Data from SAGE Web Services\n",
    "\n",
    "The fdsnws-dataselect service provides access to time series data for specified channels and time ranges. Dataselect implements the [FDSN web service specification](https://www.fdsn.org/webservices/).\n",
    "\n",
    "Data queries use SEED time series identifiers (network, station, location & channel) in addition to time ranges. Data are returned in miniSEED , SAC zip , and GeoCSV format.\n",
    "\n",
    "To create a request the Dataselect API takes these parameters at a minimum:\n",
    "\n",
    "| parameters | examples | discussion | default |type |\n",
    "| ---------- | -------- | ---------- | ------- |-----|\n",
    "| start[time] |\t2010-02-27T06:30:00\t| Specifies the desired start-time for miniSEED data | | day/time |\n",
    "| end[time]\t| 2010-02-27T10:30:00 | Specify the end-time for the miniSEED data | | day/time | \n",
    "|net[work] | IU | Select one or more network codes. Accepts wildcards and lists. Can be SEED codes or data center defined codes. | any | string |\n",
    "| sta[tion] | ANMO | Select one or more SEED station codes. Accepts wildcards and lists. | any| string |\n",
    "|loc[ation] |00 | Select one or more SEED location identifier. Accepts wildcards and lists. Use -- for “Blank” location IDs (ID’s containing 2 spaces). | any | string |\n",
    "| cha[nnel] | BHZ | Select one or more SEED channel codes. Accepts wildcards and lists. | any | string |\n",
    "\n",
    "To download a file, we can use the `requests` package send the HTTP request to the dataselect web service. As discussed in the previous section, request must include a authorization token which can be acquired with the `get_token` function.\n",
    "\n",
    "The `download_data` function requires the query parameters required by the FDSN Web Service specification and where to write the data. The function does several things. First, it requests a authorization token. Next, it creates a file name for the data. Finally, it makes the request to dataselect and writes the data to a file to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36e1f63-bc44-49fd-8876-1e501731ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from earthscope_sdk import EarthScopeClient\n",
    "\n",
    "# SAGE archive\n",
    "URL = \"http://service.iris.edu/fdsnws/dataselect/1/query?\"\n",
    "\n",
    "# function to get authorization token \n",
    "def get_token(token_path='./'):\n",
    "    \n",
    "    # refresh the token if it has expired\n",
    "    client.ctx.auth_flow.refresh_if_necessary()\n",
    "\n",
    "    token = client.ctx.auth_flow.access_token\n",
    "    \n",
    "    return token\n",
    "\n",
    "def download_data(params, data_directory):\n",
    "\n",
    "    # get authorization Bearer token\n",
    "    token = get_token()\n",
    "\n",
    "    # get year and day from string start time\n",
    "    start_date = datetime.strptime(params['start'], '%Y-%m-%dT%H:%M:%S')\n",
    "    year = start_date.year\n",
    "    day = start_date.day\n",
    "    \n",
    "    \n",
    "    # file name format: STATION.NETWORK.YEAR.DAYOFYEAR\n",
    "    file_name = \".\".join([params[\"sta\"], params[\"net\"],params['loc'],params['cha'], str(year), \"{:03d}\".format(day),'mseed'])\n",
    "    \n",
    "    \n",
    "    r = requests.get(URL, params=params, headers={\"authorization\": f\"Bearer {token}\"}, stream=True)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        # save the file\n",
    "        with open(Path(Path(data_directory) / file_name), 'wb') as f:\n",
    "            for data in r:\n",
    "                f.write(data)\n",
    "    else:\n",
    "        #problem occured\n",
    "        print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "        \n",
    "\n",
    "# create client to get token\n",
    "client = EarthScopeClient()\n",
    "\n",
    "# create directory for data\n",
    "data_directory = \"./miniseed_data\"\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "# parameters specifying the miniSEED file\n",
    "params = {\"net\" : 'IU',\n",
    "          \"sta\" : 'ANMO',\n",
    "          \"loc\" : '00',\n",
    "          \"cha\" : 'BHZ',\n",
    "          \"start\": '2010-02-27T06:30:00',\n",
    "          \"end\": '2010-02-27T10:30:00'}\n",
    "\n",
    "download_data(params, data_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6ecef-1c50-4a8f-bc9f-c56b07ef16f4",
   "metadata": {},
   "source": [
    "## Getting Geodetic Data from GAGE Web Services\n",
    "\n",
    "The GAGE archive holds many types of data ranging from GPS/GNSS data to borehole strain data. We will focus on GPS/GNSS data. Each type of data has API interfaces specific to the data. Unlike dataselect, the API calls return information about data or processed data. The collected data is distributed by a file server and can be programatically downloaded if you know the URL to the file.\n",
    "\n",
    "In this example, we will download GNSS data in RINEX. GAGE data is located on a file server and data cab be downloaded with a properly formatted URL. The script downloads the stations by providing the parameters that make up the URL to the data. \n",
    "\n",
    "The GAGE base URL for gnss data in RINEX is `https://gage-data.earthscope.org/archive/gnss/rinex/obs/`. \n",
    "\n",
    "Files are organized by year and the day of the year, e.g., `/2025/001/`. File names use this pattern: \n",
    "\n",
    "| station | day of year | 0. | two digit year | o.Z or d.Z |\n",
    "|---------|-------------|----|----------------|-----|\n",
    "| p034 | 001 |0. | 25 | d.Z |\n",
    "| p034 | 001 |0. | 25 | o.Z |\n",
    "\n",
    "The complete URL for this RINEX file:\n",
    "\n",
    "`https://gage-data.earthscope.org/archive/gnss/rinex/obs/2025/001/p0340010.25d.Z`\n",
    "\n",
    "> Note: files ending with `d.Z` are [hatanaka compressed files](https://www.unavco.org/data/gps-gnss/hatanaka/hatanaka.html) and files ending with `o.Z` are not hatanaka compressed. Hatanaka compressed files are much smaller but require software to read the data.\n",
    "\n",
    "The same method for downloading SAGE data can be used to download GAGE data once URL is properly constructed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1393025c-5e49-4460-8277-f1115d24c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from pathlib import Path\n",
    "from earthscope_sdk import EarthScopeClient\n",
    "\n",
    "client = EarthScopeClient()\n",
    "\n",
    "BASE_URL= 'https://gage-data.earthscope.org/archive/gnss/rinex/obs/'\n",
    "\n",
    "# function to get authorization token \n",
    "def get_token(token_path='./'):\n",
    "    \n",
    "    # refresh the token if it has expired\n",
    "    client.ctx.auth_flow.refresh_if_necessary()\n",
    "\n",
    "    token = client.ctx.auth_flow.access_token\n",
    "    \n",
    "    return token\n",
    "\n",
    "# function to download data from GAGE archive\n",
    "def download_file(url, data_directory):\n",
    "    \n",
    "    # get authorization Bearer token\n",
    "    token = get_token()\n",
    "\n",
    "    # the pathlib package (https://docs.python.org/3/library/pathlib.html#accessing-individual-parts) \n",
    "    # supports extracting the file name from the end of a path\n",
    "    file_name = Path(url).name\n",
    "    \n",
    "    # request a file and provide the token in the Authorization header\n",
    "    r = requests.get(url, headers={\"authorization\": f\"Bearer {token}\"}, stream=True)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        # save the file\n",
    "        with open(Path(Path(data_directory) / file_name), 'wb') as f:\n",
    "            for data in r:\n",
    "                f.write(data)\n",
    "    else:\n",
    "        #problem occured\n",
    "        print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "\n",
    "# function to creat URL to download data\n",
    "def create_url(year, day, station, compression):\n",
    "    # using Python string formatting and slicing\n",
    "    doy = '%03d' % (day) # converts day to a three character zero padded string , '001'\n",
    "    two_digit_year = str(year)[2:] # converts integer to string and slices the last characters\n",
    "\n",
    "    # using the Python join method to concatenate an array or list of strings\n",
    "    file_path = '/'.join([str(year), doy]) # integer year converted to string for string join\n",
    "    file_name = ''.join(['/', station, doy, '0.', two_digit_year, compression])\n",
    "    url = ''.join([BASE_URL,file_path,file_name])\n",
    "\n",
    "    return url\n",
    "\n",
    "# create a directory for rinex data\n",
    "directory_path = \"./rinex_data\"\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "# data requested from station p034 on January 1, 2025 hatanaka compressed\n",
    "year = 2025\n",
    "day = 1\n",
    "station = 'p034'\n",
    "compression = 'd.Z'\n",
    "\n",
    "# download the RINEX file\n",
    "url = create_url(year, day, station, compression)\n",
    "download_file(url, directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c80fd-9a93-4900-9b59-a426372e4c04",
   "metadata": {},
   "source": [
    "In this example, we've added a function to create a URL to the data. While this can be done more succintly in a single line, the example demonstrates how to format parameters using Python string functions and how to join strings to form a URL.\n",
    "\n",
    "More succint code would form the URL would be to use string formatting and add the following code to `download_file` function along with the required parameters. However, this is less explicit.\n",
    "\n",
    "```\n",
    "doy = '%03d'.format(day)\n",
    "two_digit_year = str(year)[2:]\n",
    "url='https://gage-data.earthscope.org/archive/gnss/rinex/obs/{}/{}/{}{}.{}d.Z'.format(year,doy,station,doy,two_digit_year)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a20e77-5ccc-4d96-bd7a-19a4560c7379",
   "metadata": {},
   "source": [
    "## Data Access to AWS S3\n",
    "\n",
    "![](images/cloud_native_data_access.png)\n",
    "\n",
    "Object storage in the cloud is a cost effective way to hold and distribute large collections of data. Objects consist of the data, metadata, and a unique identifier. They are accessed through an application programming interface or API. EarthScope uses Amazon Web Services' (AWS) Simple Storage Service or S3 to store and distribute seismic and geodetic data.\n",
    "\n",
    "AWS S3 supports streaming data directly into memory. This is a major advantage when analyzing large amounts of data because writing and reading data to and from a drive consumes the majority of time when performing an analysis. When data is streamed directly into memory, it is immediately available for processing.\n",
    "\n",
    "### Buckets and Keys\n",
    "\n",
    "Objects in S3 are stored in containers called `buckets`. Each object is identified by unique object identifier, or `keys`. Objects are addressed by a combination of the web service endpoint, a bucket name, and a key. Unlike a hierarchical file system on your computer, S3 doesn't have directories, instead it has prefixes which act as filters that logically groups data. Consider the following example, we can decipher the key:\n",
    "\n",
    "> s3:ncedc-pds/continuous_waveforms/BK/2022/2022.231/MERC.BK.HNZ.00.D.2022.231\n",
    "\n",
    "- s3 - service name\n",
    "- ncedc-pds - bucket name\n",
    "- continuous_waveforms - prefix\n",
    "- BK - (prefix) seismic network name \n",
    "- 2022 - (prefix) year \n",
    "- 2022.231 - (prefix) year and day of year\n",
    "- MERC.BK.HNZ.00.D.2022.231 - (key) station.network.channel.location.year.day of year\n",
    "\n",
    "Like a web service file URL, the object key is used to request the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6585f-15e9-4fcc-b8a5-fa76afbec1c5",
   "metadata": {},
   "source": [
    "### S3 Buckets with Public Read Access\n",
    "\n",
    "S3 buckets can be configured for public read access, you can access objects without providing credentials. The [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) Python package provides libraries for working with AWS services, including S3. Boto3 provides two methods for interacting with AWS services. The `client` method is a low level and fine-grained interface that closely follows the AWS API for a service. The 'resource` method is a high level interface that wraps the `client` interface. AWS stopped development on the resource interface in `boto3` in 2023, for this reason we will use the `client` interface when working with S3 resources.\n",
    "\n",
    "The following example reads a miniSEED file from the Northern California Earthquake Data Center (NCEDC). The trace data is for the 2014 Napa earthquake. GeoLab's default environment includes both `boto3` and `obspy` packages and we can import them without installation. We establish the connection to S3 by creating a client that specifies that requests are unsigned. This means that the S3 bucket allows public access and does not require credentials. The client calls the `get_object` method with the bucket name and key for the miniSEED object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d8d38fc-7693-472d-b5bd-c760ed9ac71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Trace(s) in Stream:\n",
      "BK.PACP.00.HHN | 2014-08-24T00:00:00.008393Z - 2014-08-24T23:59:59.998393Z | 100.0 Hz, 8640000 samples\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "# from io import BytesIO\n",
    "import io\n",
    "from obspy import read\n",
    "\n",
    "s3 = boto3.client('s3', config = Config(signature_version = UNSIGNED), region_name='us-west-2')\n",
    "\n",
    "BUCKET_NAME = 'ncedc-pds'\n",
    "KEY = 'continuous_waveforms/BK/2014/2014.236/PACP.BK.HHN.00.D.2014.236'\n",
    "\n",
    "response = s3.get_object(Bucket=BUCKET_NAME, Key=KEY)\n",
    "data_stream = io.BytesIO(response['Body'].read())\n",
    "\n",
    "# Parse with ObsPy\n",
    "st = read(data_stream)\n",
    "\n",
    "# Print the ObsPy Streams\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dce4a-c08b-42d8-8e28-262fc0ffaba4",
   "metadata": {},
   "source": [
    "### AWS Temporary Credentials (to be implemented)\n",
    "\n",
    "Like EarthScope web services, you will need an EarthScope login to request and authorization token. The token can be used to request temporary AWS credentials. AWS services require credentials to interact with AWS S3, or any other AWS service. Requests are cryptographically signed using AWS credentials (access key ID, secret access key, and optionally a session token). In the near future, EarthScope will issue temporary credentials that will allow you to access SAGE and GAGE data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
